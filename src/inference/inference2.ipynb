{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from src.config.core import Config\n",
    "from src.config.sampler import Sampler\n",
    "from src.config.data import DatasetType\n",
    "import src.dataset as ds\n",
    "import src.training.utils as train_utils\n",
    "import src.inference.utils as inf_utils\n",
    "import src.visualization as viz\n",
    "from src.config.data import Task\n",
    "from src.inference.evaluation import evaluate_bde\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    DIR = Path(os.environ['SANDBOX_EXPERIMENT_DIR'])\n",
    "except KeyError as e:\n",
    "    raise KeyError('Please set the \"SANDBOX_EXPERIMENT_DIR\" variable') from e\n",
    "\n",
    "tree_dir = DIR / 'tree'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Log Summary</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_time(logs: str):\n",
    "    log_lines = [line for line in logs.split('\\n') if line]\n",
    "    # Extract Timestamp\n",
    "    if len(log_lines) < 2:\n",
    "        return 0 \n",
    "    start = re.findall(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', log_lines[0])[0]\n",
    "    end = re.findall(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', log_lines[-1])[0]\n",
    "    return (datetime.fromisoformat(end) - datetime.fromisoformat(start)).total_seconds()\n",
    "\n",
    "with open(DIR / 'training.log', 'r') as f:\n",
    "    logs = f.read()\n",
    "\n",
    "warm_incomplete = False\n",
    "bde_incomplete = False\n",
    "\n",
    "warmstart_match = re.findall(r'time.warmstart took (\\d+.\\d+) seconds', logs)\n",
    "bde_match = re.findall(r'time.sampling took (\\d+.\\d+) seconds', logs)\n",
    "if warmstart_match:\n",
    "    warmstart_time = float(warmstart_match[0])\n",
    "    if bde_match:\n",
    "        bde_time = float(bde_match[0])\n",
    "    else:\n",
    "        bde_time = get_total_time(logs) - warmstart_time\n",
    "        bde_incomplete = True\n",
    "else:\n",
    "    warm_incomplete = True\n",
    "    bde_incomplete = True\n",
    "    warmstart_time = get_total_time(logs)\n",
    "    bde_time = 0\n",
    "\n",
    "total_time = round(warmstart_time + bde_time, 2)\n",
    "print(\"-\" * 50)\n",
    "print(f'Warmstart time: {round(warmstart_time / 60, 2)} min{\" (incomplete)\" if warm_incomplete else \"\"}')\n",
    "print(f'BDE time: {round(bde_time / 60, 2)} min{\" (incomplete)\" if bde_incomplete else \"\"}')\n",
    "print(f'Total time: {round(total_time / 60, 2)} min{\" (incomplete)\" if warm_incomplete or bde_incomplete else \"\"}')\n",
    "print(\"-\" * 50)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    config = Config.from_yaml(DIR / 'config.yaml')\n",
    "# print(f\"> Loaded Configuration:\\n{config}\")\n",
    "metrics = {\n",
    "    'total_time': total_time,\n",
    "    'warmstart_time': warmstart_time,\n",
    "    'bde_time': bde_time\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Sampling</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_utils.load_samples_from_dir(DIR / 'samples', tree_path=tree_dir)\n",
    "n_samples = inf_utils.count_samples(samples)\n",
    "n_chains = inf_utils.count_chains(samples)\n",
    "print(f'Loaded {n_chains} chains with {n_samples} samples each')\n",
    "mean_ess = viz.plot_effective_sample_size(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_bcv, mean_wcv = viz.plot_variances(samples)\n",
    "# mean_crhat = viz.plot_split_chain_r_hat(samples, n_splits=4)\n",
    "# viz.plot_param_hist(samples, hist_limit=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz.plot_pca(samples, '3d', max_chains=2, max_samples=None, annotate=True)\n",
    "viz.plot_param_movement(samples, random_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine the mean diagnostics into a dataframe and save it\n",
    "# rows = list(mean_ess.keys())\n",
    "# columns = ['ess', 'bcv', 'wcv', 'crhat']\n",
    "# df = pd.DataFrame(index=rows, columns=columns)\n",
    "# df['ess'] = mean_ess.values()\n",
    "# df['bcv'] = mean_bcv.values()\n",
    "# df['wcv'] = mean_wcv.values()\n",
    "# df['crhat'] = mean_crhat.values()\n",
    "# df.to_csv(DIR / 'diagnostics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>BDE Performance</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra arguments needed in forward pass of the flax model\n",
    "kwargs = {}\n",
    "\n",
    "# Setup Loader\n",
    "if config.data.data_type == DatasetType.TABULAR:\n",
    "    loader = ds.TabularLoader(\n",
    "        config.data,\n",
    "        rng=config.jax_rng,\n",
    "        target_len=config.data.target_len\n",
    "    )\n",
    "elif config.data.data_type == DatasetType.TEXT:\n",
    "    from src.dataset.utils import CustomBPETokenizer\n",
    "    tokenizer = CustomBPETokenizer(\n",
    "        config.model.context_len, max_vocab_size=config.model.vocab_size\n",
    "    )\n",
    "    loader = ds.TextLoader(\n",
    "        config=config.data,\n",
    "        rng=config.jax_rng,\n",
    "        tokenizer=tokenizer,\n",
    "        context_len=config.model.context_len,\n",
    "        omit_freq=100,\n",
    "    )\n",
    "    kwargs = {'train': False, 'pad_id': tokenizer.padding_token_id}\n",
    "elif config.data.data_type == DatasetType.IMAGE:\n",
    "    loader = ds.ImageLoader(\n",
    "        config.data,\n",
    "        rng=config.jax_rng,\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError(f'DatasetType {config.data.data_type} not supported yet.')\n",
    "\n",
    "random_input = next(loader.iter('train', 1))['feature']\n",
    "\n",
    "print(f'> Loader:\\n{loader}')\n",
    "module = config.get_flax_model()\n",
    "\n",
    "print('> Parameter Overview:\\n')\n",
    "print(module.tabulate(config.jax_rng, x=random_input, **kwargs))\n",
    "\n",
    "with open(DIR / 'samples/info.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "\n",
    "accept_rate = info.get(\"acceptance_rate\", None)\n",
    "integration_steps = info.get(\"num_integration_steps\", None)\n",
    "if accept_rate is not None:\n",
    "    print(\"Average acceptance rate: \", accept_rate.mean())\n",
    "    metrics['acceptance_rate'] = accept_rate.mean()\n",
    "if integration_steps is not None:\n",
    "    print(\"Average ingeration steps: \", integration_steps.mean())\n",
    "    metrics['integration_steps'] = integration_steps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = loader.test_x # (B x F)\n",
    "labels = loader.test_y # (B x T)\n",
    "print(\"Test Set: Feature and Label have shapes: \", features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits, metrics = evaluate_de(\n",
    "#     params=params_warmstart,\n",
    "#     module=module,\n",
    "#     features=features,\n",
    "#     labels=labels,\n",
    "#     task=config.data.task,\n",
    "#     batch_size=config.training.warmstart.batch_size,\n",
    "#     verbose=False,\n",
    "#     metrics_dict=metrics,\n",
    "#     n_samples=config.training.sampler.n_samples // config.training.sampler.n_thinning,\n",
    "#     rng_key=config.jax_rng,\n",
    "#     nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_cap = 4 * 1024 ** 3 # 4 GB\n",
    "mem_usage = (\n",
    "    inf_utils.get_mem_size(samples) / n_chains\n",
    "    + inf_utils.get_mem_size(features)\n",
    "    + inf_utils.get_mem_size(labels)\n",
    ")\n",
    "overhead_unit = mem_usage * 10 # estimatation\n",
    "\n",
    "batch_size = max(1, (mem_cap - overhead_unit) // mem_usage)\n",
    "if batch_size > labels.shape[0]:\n",
    "    batch_size = None\n",
    "print(f'> Batch size for evaluation: {batch_size}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, metrics = evaluate_bde(\n",
    "    params=samples,\n",
    "    module=module,\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    task=config.data.task,\n",
    "    batch_size=batch_size,\n",
    "    verbose=True,\n",
    "    metrics_dict=metrics,\n",
    "    nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPPD over the samples\n",
    "viz.plot_lppd(logits, labels, config.data.task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel vs. Sequential (NOT HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(DIR / 'samples/info.pkl', 'rb') as f:\n",
    "#     info = pickle.load(f)\n",
    "\n",
    "# n_cycles = info['scheduler_config']['parameters']['n_cycles']\n",
    "# cycle_length = n_samples // n_cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_sizes = np.array(info['step_size'])\n",
    "# explore = np.array(info['explore'])\n",
    "# sampling_points = np.ma.masked_where(explore, step_sizes)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(step_sizes, lw=2, ls=\"--\", color=\"r\", label=\"Exploration stage\")\n",
    "# ax.plot(sampling_points, lw=2, ls=\"-\", color=\"k\", label=\"Sampling stage\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel = jax.tree.map(\n",
    "#     lambda x: x[:, :cycle_length, :],\n",
    "#     samples\n",
    "# )\n",
    "\n",
    "# print('Parallel')\n",
    "# logits_par, metrics_par = evaluate_bde(\n",
    "#     params=parallel,\n",
    "#     module=module,\n",
    "#     features=features,\n",
    "#     labels=labels,\n",
    "#     task=config.data.task,\n",
    "#     batch_size=batch_size,\n",
    "#     verbose=False,\n",
    "#     metrics_dict=metrics.copy(),\n",
    "#     nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "#     per_chain=False,\n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_seq = []\n",
    "# lppds = []\n",
    "# rmses = []\n",
    "# print('Sequential')\n",
    "# for i in range(n_chains):\n",
    "#     sequential = jax.tree.map(\n",
    "#         lambda x: x[i, :, :][None, ...],\n",
    "#         samples\n",
    "#     )\n",
    "#     print('=' * 50)\n",
    "#     print(f'Chain {i}')\n",
    "#     logits, metrics = evaluate_bde(\n",
    "#         params=sequential,\n",
    "#         module=module,\n",
    "#         features=features,\n",
    "#         labels=labels,\n",
    "#         task=config.data.task,\n",
    "#         batch_size=batch_size,\n",
    "#         verbose=False,\n",
    "#         metrics_dict=metrics,\n",
    "#         nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "#         **kwargs\n",
    "#     )\n",
    "#     print('=' * 50 + '\\n')\n",
    "#     # logits_seq.append(logits)\n",
    "#     # metrics_seq.append(metrics)\n",
    "#     lppds.append(metrics['lppd'])\n",
    "#     rmses.append(metrics['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lppds = jnp.array(lppds)\n",
    "# rmses = jnp.array(rmses)\n",
    "\n",
    "# lppd_seq_mean = jnp.array(lppds).mean()\n",
    "# rmse_seq_mean = jnp.array(rmses).mean()\n",
    "\n",
    "# lppd_par = metrics_par['lppd']\n",
    "# rmse_par = metrics_par['rmse']\n",
    "\n",
    "# # print(f'Parallel LPPD: {lppd_par:.4f}, Sequential LPPD: {lppd_seq_mean:.4f}')\n",
    "# # print(f'Parallel RMSE: {rmse_par:.4f}, Sequential RMSE: {rmse_seq_mean:.4f}')\n",
    "\n",
    "# # Summary table\n",
    "# data = {\n",
    "#     \"Chain\": [f\"Chain {i}\" for i in range(len(lppds))] + [\"Sequential Mean\", \"Parallel\"],\n",
    "#     \"LPPD\": list(lppds) + [lppd_seq_mean, lppd_par],\n",
    "#     \"RMSE\": list(rmses) + [rmse_seq_mean, rmse_par],\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "# pickle.dump(df, open(DIR / 'summary.pkl', 'wb'))\n",
    "# df.style.format({\n",
    "#     \"LPPD\": \"{:.4f}\",\n",
    "#     \"RMSE\": \"{:.4f}\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DIR / \"metrics.csv\")\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(8, 8), nrows=2)\n",
    "for i in range(n_chains):\n",
    "    chain_df = df[df[\"chain\"] == i]\n",
    "    axs[0].plot(chain_df[\"step_count\"], chain_df[\"log_posterior\"], label=f\"Chain {i}\")\n",
    "    axs[1].plot(chain_df[\"step_count\"], chain_df[\"rmse\"], label=f\"Chain {i}\")\n",
    "axs[0].set_title(\"Log Posterior\")\n",
    "axs[1].set_title(\"RMSE\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
