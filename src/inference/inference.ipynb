{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "\n",
    "os.chdir('/home/bszh/MILE')\n",
    "\n",
    "from src.config.core import Config\n",
    "from src.config.sampler import Sampler\n",
    "from src.config.data import DatasetType\n",
    "import src.dataset as ds\n",
    "import src.training.utils as train_utils\n",
    "import src.inference.utils as inf_utils\n",
    "import src.visualization as viz\n",
    "from src.config.data import Task\n",
    "from src.inference.evaluation import evaluate_de, evaluate_bde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SANDBOX_EXPERIMENT_DIR'] = '/home/bszh/MILE/results/bikesharing/bike_sgle1_20250610-192029'\n",
    "try:\n",
    "    DIR = Path(os.environ['SANDBOX_EXPERIMENT_DIR'])\n",
    "except KeyError as e:\n",
    "    raise KeyError('Please set the \"SANDBOX_EXPERIMENT_DIR\" variable') from e\n",
    "\n",
    "tree_dir = DIR / 'tree'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Log Summary</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_time(logs: str):\n",
    "    log_lines = [line for line in logs.split('\\n') if line]\n",
    "    # Extract Timestamp\n",
    "    if len(log_lines) < 2:\n",
    "        return 0 \n",
    "    start = re.findall(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', log_lines[0])[0]\n",
    "    end = re.findall(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', log_lines[-1])[0]\n",
    "    return (datetime.fromisoformat(end) - datetime.fromisoformat(start)).total_seconds()\n",
    "\n",
    "with open(DIR / 'training.log', 'r') as f:\n",
    "    logs = f.read()\n",
    "\n",
    "warm_incomplete = False\n",
    "bde_incomplete = False\n",
    "\n",
    "warmstart_match = re.findall(r'time.warmstart took (\\d+.\\d+) seconds', logs)\n",
    "bde_match = re.findall(r'time.sampling took (\\d+.\\d+) seconds', logs)\n",
    "if warmstart_match:\n",
    "    warmstart_time = float(warmstart_match[0])\n",
    "    if bde_match:\n",
    "        bde_time = float(bde_match[0])\n",
    "    else:\n",
    "        bde_time = get_total_time(logs) - warmstart_time\n",
    "        bde_incomplete = True\n",
    "else:\n",
    "    warm_incomplete = True\n",
    "    bde_incomplete = True\n",
    "    warmstart_time = get_total_time(logs)\n",
    "    bde_time = 0\n",
    "\n",
    "total_time = round(warmstart_time + bde_time, 2)\n",
    "print(\"-\" * 50)\n",
    "print(f'Warmstart time: {round(warmstart_time / 60, 2)} min{\" (incomplete)\" if warm_incomplete else \"\"}')\n",
    "print(f'BDE time: {round(bde_time / 60, 2)} min{\" (incomplete)\" if bde_incomplete else \"\"}')\n",
    "print(f'Total time: {round(total_time / 60, 2)} min{\" (incomplete)\" if warm_incomplete or bde_incomplete else \"\"}')\n",
    "print(\"-\" * 50)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    config = Config.from_yaml(DIR / 'config.yaml')\n",
    "# print(f\"> Loaded Configuration:\\n{config}\")\n",
    "metrics = {\n",
    "    'total_time': total_time,\n",
    "    'warmstart_time': warmstart_time,\n",
    "    'bde_time': bde_time\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmstart Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmstart_dir = DIR / 'warmstart'\n",
    "files = [warmstart_dir / f for f in os.listdir(warmstart_dir) if f.endswith('.npz')]\n",
    "params_warmstart = train_utils.load_params_batch(files, tree_path=tree_dir)\n",
    "if len(files) == 1:\n",
    "    params_warmstart = jax.tree.map(lambda x: x[None, ...], params_warmstart)\n",
    "print(f'Number of NaNs:\\n{inf_utils.count_nan(params_warmstart)}')\n",
    "viz.plot_warmstart_results(warmstart_dir=warmstart_dir)\n",
    "# viz.plot_param_hist(params_warmstart, hist_=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Sampling Warmup</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.training.sampler.name == Sampler.MCLMC:\n",
    "    with open(DIR / 'warmup_params.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        step_size = jax.numpy.array([float(x) for x in lines[0].strip().split(',')])\n",
    "        L = jax.numpy.array([float(x) for x in lines[1].strip().split(',')])\n",
    "\n",
    "    print('Tuned MCLMC Parameters after warmup:')\n",
    "    print(f'Step Size: {jax.numpy.mean(step_size):.4f} ± {jax.numpy.std(step_size):.4f}')\n",
    "    print(f'L: {jax.numpy.mean(L):.4f} ± {jax.numpy.std(L):.4f}')\n",
    "    metrics['step_size'] = jax.numpy.mean(step_size)\n",
    "    metrics['L'] = jax.numpy.mean(L)\n",
    "    metrics['step_size_std'] = jax.numpy.std(step_size)\n",
    "    metrics['L_std'] = jax.numpy.std(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit memory usage\n",
    "if config.training.sampler.keep_warmup:\n",
    "    samples_warmup = train_utils.load_samples_from_dir(DIR / 'sampling_warmup', tree_path=tree_dir)\n",
    "    n_samples = inf_utils.count_samples(samples_warmup)\n",
    "    n_chains = inf_utils.count_chains(samples_warmup)\n",
    "    print(f'Loaded {n_chains} chains with {n_samples} warmup samples each')\n",
    "    print(f'Number of NaNs:\\n{inf_utils.count_nan(samples_warmup)}')\n",
    "    _ = viz.plot_effective_sample_size(samples_warmup)\n",
    "    viz.plot_param_movement(samples_warmup, random_n=5)\n",
    "    # viz.plot_param_hist(samples_warmup, hist_limit=10000)\n",
    "else:\n",
    "    print('> No warmup samples has been saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Sampling</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = train_utils.load_samples_from_dir(DIR / 'samples', tree_path=tree_dir)\n",
    "n_samples = inf_utils.count_samples(samples)\n",
    "n_chains = inf_utils.count_chains(samples)\n",
    "print(f'Loaded {n_chains} chains with {n_samples} samples each')\n",
    "mean_ess = viz.plot_effective_sample_size(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bcv, mean_wcv = viz.plot_variances(samples)\n",
    "mean_crhat = viz.plot_split_chain_r_hat(samples, n_splits=4)\n",
    "# viz.plot_param_hist(samples, hist_limit=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_pca(samples, '3d', max_chains=2, max_samples=None, annotate=True)\n",
    "viz.plot_param_movement(samples, random_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the mean diagnostics into a dataframe and save it\n",
    "rows = list(mean_ess.keys())\n",
    "columns = ['ess', 'bcv', 'wcv', 'crhat']\n",
    "df = pd.DataFrame(index=rows, columns=columns)\n",
    "df['ess'] = mean_ess.values()\n",
    "df['bcv'] = mean_bcv.values()\n",
    "df['wcv'] = mean_wcv.values()\n",
    "df['crhat'] = mean_crhat.values()\n",
    "df.to_csv(DIR / 'diagnostics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>DE vs BDE Performance</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra arguments needed in forward pass of the flax model\n",
    "kwargs = {}\n",
    "\n",
    "# Setup Loader\n",
    "if config.data.data_type == DatasetType.TABULAR:\n",
    "    loader = ds.TabularLoader(\n",
    "        config.data,\n",
    "        rng=config.jax_rng,\n",
    "        target_len=config.data.target_len\n",
    "    )\n",
    "elif config.data.data_type == DatasetType.TEXT:\n",
    "    from src.dataset.utils import CustomBPETokenizer\n",
    "    tokenizer = CustomBPETokenizer(\n",
    "        config.model.context_len, max_vocab_size=config.model.vocab_size\n",
    "    )\n",
    "    loader = ds.TextLoader(\n",
    "        config=config.data,\n",
    "        rng=config.jax_rng,\n",
    "        tokenizer=tokenizer,\n",
    "        context_len=config.model.context_len,\n",
    "        omit_freq=100,\n",
    "    )\n",
    "    kwargs = {'train': False, 'pad_id': tokenizer.padding_token_id}\n",
    "elif config.data.data_type == DatasetType.IMAGE:\n",
    "    loader = ds.ImageLoader(\n",
    "        config.data,\n",
    "        rng=config.jax_rng,\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError(f'DatasetType {config.data.data_type} not supported yet.')\n",
    "\n",
    "random_input = next(loader.iter('train', 1))['feature']\n",
    "\n",
    "print(f'> Loader:\\n{loader}')\n",
    "module = config.get_flax_model()\n",
    "\n",
    "print('> Parameter Overview:\\n')\n",
    "print(module.tabulate(config.jax_rng, x=random_input, **kwargs))\n",
    "\n",
    "with open(DIR / 'samples/info.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "\n",
    "accept_rate = info.get(\"acceptance_rate\", None)\n",
    "integration_steps = info.get(\"num_integration_steps\", None)\n",
    "if accept_rate is not None:\n",
    "    print(\"Average acceptance rate: \", accept_rate.mean())\n",
    "    metrics['acceptance_rate'] = accept_rate.mean()\n",
    "if integration_steps is not None:\n",
    "    print(\"Average ingeration steps: \", integration_steps.mean())\n",
    "    metrics['integration_steps'] = integration_steps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = loader.test_x # (B x F)\n",
    "labels = loader.test_y # (B x T)\n",
    "print(\"Test Set: Feature and Label have shapes: \", features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, metrics = evaluate_de(\n",
    "    params=params_warmstart,\n",
    "    module=module,\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    task=config.data.task,\n",
    "    batch_size=config.training.warmstart.batch_size,\n",
    "    verbose=False,\n",
    "    metrics_dict=metrics,\n",
    "    n_samples=config.training.sampler.n_samples // config.training.sampler.n_thinning,\n",
    "    rng_key=config.jax_rng,\n",
    "    nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_cap = 4 * 1024 ** 3 # 4 GB\n",
    "mem_usage = (\n",
    "    inf_utils.get_mem_size(samples) / n_chains\n",
    "    + inf_utils.get_mem_size(features)\n",
    "    + inf_utils.get_mem_size(labels)\n",
    ")\n",
    "overhead_unit = mem_usage * 10 # estimatation\n",
    "\n",
    "batch_size = max(1, (mem_cap - overhead_unit) // mem_usage)\n",
    "if batch_size > labels.shape[0]:\n",
    "    batch_size = None\n",
    "print(f'> Batch size for evaluation: {batch_size}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, metrics = evaluate_bde(\n",
    "    params=samples,\n",
    "    module=module,\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    task=config.data.task,\n",
    "    batch_size=batch_size,\n",
    "    verbose=False,\n",
    "    metrics_dict=metrics,\n",
    "    nominal_coverages=[0.5, 0.75, 0.9, 0.95],\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LPPD over the samples\n",
    "viz.plot_lppd(logits, labels, config.data.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics\n",
    "with open(DIR / 'metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
