{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b0c8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-17 21:20:52,968 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:20:52,970 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:20:53,371 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:20:53,372 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:20:53,373 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:20:53,373 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/bszh/MILE'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import warnings\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from src.config.core import Config\n",
    "from src.config.sampler import Sampler\n",
    "from src.config.data import DatasetType\n",
    "import src.dataset as ds\n",
    "from src.models.tabular import FCN\n",
    "import src.training.utils as train_utils\n",
    "import src.inference.utils as inf_utils\n",
    "import src.visualization as viz\n",
    "from src.config.data import Task\n",
    "from src.inference.evaluation import evaluate_bde\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "DIR = os.getcwd()\n",
    "DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15a5339",
   "metadata": {},
   "source": [
    "### Config template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "915b8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DICT = {\n",
    "    'saving_dir': 'results/',\n",
    "    'experiment_name': 'bike',\n",
    "    'data': {\n",
    "        'path': 'data/bikesharing.data',\n",
    "        'source': 'local',\n",
    "        'data_type': 'tabular',\n",
    "        'task': 'regr',\n",
    "        'target_column': None,\n",
    "        'target_len': 1,\n",
    "        'features': None,\n",
    "        'datapoint_limit': None,\n",
    "        'normalize': True,\n",
    "        'train_split': 0.7,\n",
    "        'valid_split': 0.1,\n",
    "        'test_split': 0.2,\n",
    "    },\n",
    "    'model': {\n",
    "        'model': 'FCN',\n",
    "        'hidden_structure': [16, 16, 16, 2],\n",
    "        'activation': 'relu',\n",
    "        'use_bias': True,\n",
    "    },\n",
    "    'training': {\n",
    "        'warmstart': { # meaningless placeholder\n",
    "            'include': False,\n",
    "            'optimizer_config': {'name': \"sgd\", 'parameters': {}}\n",
    "        },\n",
    "        'sampler': {\n",
    "            'name': 'sgld',\n",
    "            'warmup_steps': 0,\n",
    "            'n_chains': 4,\n",
    "            'n_samples': 24000,  # total steps\n",
    "            'batch_size': 512,\n",
    "            'step_size_init': 2.0e-6,  # step_size_explore\n",
    "            'n_thinning': 1,\n",
    "            'keep_warmup': False,\n",
    "            'optimizer_name': 'sgd',\n",
    "            'prior_config': {\n",
    "                'name': 'StandardNormal'\n",
    "            },\n",
    "            'scheduler_config': {\n",
    "                'name': 'Cyclical',\n",
    "                'n_samples_per_cycle': 200,\n",
    "                'parameters': {\n",
    "                    'n_cycles': 4,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'rng': 1446,\n",
    "    'logging': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1615637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(\n",
    "        exp_name: str = 'bike',\n",
    "        n_chains: int = 4,\n",
    "        n_cycles: int = 4,\n",
    "        n_steps_per_cycle: int = 2000,\n",
    "        n_samples_per_cycle: int = 200,\n",
    "        n_thinning: int = 1,\n",
    "        optimizer_name: str = 'adam',\n",
    "        scheduler_name: str = 'Cyclical',\n",
    "        step_size_init: float = 2.0e-6,\n",
    "        step_size_sampling: float | None = None,\n",
    "        seed: int = 0\n",
    "    ):\n",
    "    n_samples = n_cycles * n_steps_per_cycle\n",
    "\n",
    "    new_config_dict = CONFIG_DICT.copy()\n",
    "    new_config_dict['experiment_name'] = exp_name\n",
    "    new_config_dict['training']['sampler'] = {\n",
    "        'name': 'sgld',\n",
    "        'warmup_steps': 0,\n",
    "        'keep_warmup': False,\n",
    "        'n_chains': n_chains,\n",
    "        'n_samples': n_samples,  # total steps\n",
    "        'batch_size': 512,\n",
    "        'step_size_init': step_size_init,  # step_size_explore\n",
    "        'n_thinning': n_thinning,\n",
    "        'optimizer_name': optimizer_name,\n",
    "        'prior_config': {\n",
    "            'name': 'StandardNormal'\n",
    "        },\n",
    "        'scheduler_config': {\n",
    "            'name': scheduler_name,\n",
    "            'n_samples_per_cycle': n_samples_per_cycle,\n",
    "            'parameters': {\n",
    "                'n_cycles': n_cycles,\n",
    "                'step_size_sampling': step_size_sampling\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    new_config_dict['rng'] = seed\n",
    "\n",
    "    # datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    new_config_dict['saving_dir'] = f'results/'\n",
    "    config_path = Path('experiments/csgld') / f'{exp_name}.yaml'\n",
    "    if not config_path.parent.exists():\n",
    "        config_path.parent.mkdir(parents=True)\n",
    "    Config.from_dict(new_config_dict).to_yaml(config_path)\n",
    "\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cdb79",
   "metadata": {},
   "source": [
    "### Constant Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3bbc69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_2_seed0.yaml\n",
      "2025-06-17 21:20:54,104 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:20:54,639 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:20:54,640 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:20:55,029 - __main__ - INFO - > Running experiment: bike/parallel_constant_2_seed0\n",
      "2025-06-17 21:20:55,039 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:20:55,039 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:20:55,040 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:20:55,040 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:20:55,087 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:24<00:00, 101.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_2_seed1.yaml\n",
      "2025-06-17 21:21:37,971 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:21:38,572 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:21:38,572 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:21:39,026 - __main__ - INFO - > Running experiment: bike/parallel_constant_2_seed1\n",
      "2025-06-17 21:21:39,036 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:21:39,036 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:21:39,036 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:21:39,037 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:21:39,081 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:23<00:00, 104.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_2_seed2.yaml\n",
      "2025-06-17 21:22:21,085 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:22:21,609 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:22:21,609 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:22:21,982 - __main__ - INFO - > Running experiment: bike/parallel_constant_2_seed2\n",
      "2025-06-17 21:22:21,992 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:22:21,992 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:22:21,992 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:22:21,992 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:22:22,037 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:24<00:00, 103.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_4_seed0.yaml\n",
      "2025-06-17 21:23:04,344 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:23:04,864 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:23:04,864 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:23:05,233 - __main__ - INFO - > Running experiment: bike/parallel_constant_4_seed0\n",
      "2025-06-17 21:23:05,243 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:23:05,243 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:23:05,243 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:23:05,244 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:23:05,292 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:30<00:00, 83.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_4_seed1.yaml\n",
      "2025-06-17 21:23:56,203 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:23:56,736 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:23:56,737 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:23:57,120 - __main__ - INFO - > Running experiment: bike/parallel_constant_4_seed1\n",
      "2025-06-17 21:23:57,130 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:23:57,130 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:23:57,131 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:23:57,131 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:23:57,177 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:29<00:00, 83.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_4_seed2.yaml\n",
      "2025-06-17 21:24:47,630 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:24:48,181 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:24:48,182 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:24:48,558 - __main__ - INFO - > Running experiment: bike/parallel_constant_4_seed2\n",
      "2025-06-17 21:24:48,568 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:24:48,568 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:24:48,568 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:24:48,569 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:24:48,613 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:29<00:00, 83.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_8_seed0.yaml\n",
      "2025-06-17 21:25:38,966 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:25:39,504 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:25:39,505 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:25:39,871 - __main__ - INFO - > Running experiment: bike/parallel_constant_8_seed0\n",
      "2025-06-17 21:25:39,880 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:25:39,881 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:25:39,881 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:25:39,881 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:25:39,925 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:40<00:00, 62.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_8_seed1.yaml\n",
      "2025-06-17 21:26:44,724 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:26:45,247 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:26:45,248 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:26:45,633 - __main__ - INFO - > Running experiment: bike/parallel_constant_8_seed1\n",
      "2025-06-17 21:26:45,642 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:26:45,642 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:26:45,643 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:26:45,643 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:26:45,688 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:40<00:00, 62.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_8_seed2.yaml\n",
      "2025-06-17 21:27:50,426 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:27:50,958 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:27:50,959 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:27:51,346 - __main__ - INFO - > Running experiment: bike/parallel_constant_8_seed2\n",
      "2025-06-17 21:27:51,357 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:27:51,357 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:27:51,358 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:27:51,358 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:27:51,409 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:40<00:00, 62.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_12_seed0.yaml\n",
      "2025-06-17 21:28:56,272 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:28:56,792 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:28:56,793 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:28:57,159 - __main__ - INFO - > Running experiment: bike/parallel_constant_12_seed0\n",
      "2025-06-17 21:28:57,168 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:28:57,168 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:28:57,169 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:28:57,169 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:28:57,215 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:50<00:00, 49.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_12_seed1.yaml\n",
      "2025-06-17 21:30:40,598 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:30:41,131 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:30:41,132 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:30:41,512 - __main__ - INFO - > Running experiment: bike/parallel_constant_12_seed1\n",
      "2025-06-17 21:30:41,522 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:30:41,522 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:30:41,523 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:30:41,524 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:30:41,567 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:26<00:00, 93.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/parallel_constant_12_seed2.yaml\n",
      "2025-06-17 21:31:36,971 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:31:37,492 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:31:37,492 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:31:37,863 - __main__ - INFO - > Running experiment: bike/parallel_constant_12_seed2\n",
      "2025-06-17 21:31:37,873 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:31:37,873 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:31:37,874 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:31:37,874 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:31:37,916 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling: 100%|██████████| 2500/2500 [00:50<00:00, 49.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# parallel\n",
    "chains_cycles = [2, 4, 8, 12]\n",
    "config_paths = []\n",
    "for i, n_chains in enumerate(chains_cycles):\n",
    "    for seed in range(3):\n",
    "        exp_name = f'bike/parallel_constant_{n_chains}_seed{seed}'\n",
    "        config_path = get_config(\n",
    "            exp_name=exp_name,\n",
    "            n_chains=n_chains,\n",
    "            n_cycles=1,\n",
    "            n_steps_per_cycle=2500,\n",
    "            n_samples_per_cycle=500,\n",
    "            n_thinning=10,\n",
    "            optimizer_name='adam',\n",
    "            scheduler_name='Constant',\n",
    "            step_size_init=0.01,\n",
    "            step_size_sampling=1.0e-8,\n",
    "            seed=seed\n",
    "        )\n",
    "        # print(f'Config saved to {config_path}')\n",
    "        config_paths.append(config_path)\n",
    "\n",
    "for config_path in config_paths:\n",
    "    print(\"=\" * 50)\n",
    "    print(f'Running training for config: {config_path}')\n",
    "    subprocess.run(['python', 'train.py', '-c', str(config_path), '-d', '12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d6049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/sequential_constant_2_seed0.yaml\n",
      "2025-06-17 21:58:09,852 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:58:10,464 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:58:10,464 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:58:11,110 - __main__ - INFO - > Running experiment: bike/sequential_constant_2_seed0\n",
      "2025-06-17 21:58:11,120 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:58:11,120 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:58:11,121 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:58:11,121 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:58:11,172 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/5000 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bszh/MILE/train.py\", line 133, in <module>\n",
      "    train_bde(cfg, n_devices)\n",
      "  File \"/home/bszh/MILE/train.py\", line 19, in train_bde\n",
      "    trainer.train_bde()\n",
      "  File \"/home/bszh/MILE/src/training/trainer.py\", line 177, in train_bde\n",
      "    self.start_sampling()\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bszh/MILE/src/training/trainer.py\", line 599, in start_sampling\n",
      "    inference_loop_batch(\n",
      "  File \"/home/bszh/MILE/src/training/sampling_batch.py\", line 266, in inference_loop_batch\n",
      "    _log_metrics(step_count, state, batch)\n",
      "  File \"/home/bszh/MILE/src/training/sampling_batch.py\", line 217, in _log_metrics\n",
      "    curr_logpost = logpost(state.params, X, y)\n",
      "                   ^^^^^^^\n",
      "NameError: cannot access free variable 'logpost' where it is not associated with a value in enclosing scope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/sequential_constant_2_seed1.yaml\n",
      "2025-06-17 21:58:15,172 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:58:15,701 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:58:15,702 - datasets - INFO - JAX version 0.4.28 available.\n",
      "2025-06-17 21:58:16,116 - __main__ - INFO - > Running experiment: bike/sequential_constant_2_seed1\n",
      "2025-06-17 21:58:16,125 - jax._src.xla_bridge - INFO - Unable to initialize backend 'cuda': \n",
      "2025-06-17 21:58:16,126 - jax._src.xla_bridge - INFO - Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2025-06-17 21:58:16,126 - jax._src.xla_bridge - INFO - Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "2025-06-17 21:58:16,126 - jax._src.xla_bridge - WARNING - An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "2025-06-17 21:58:16,169 - src.training.trainer - INFO - > Setting up directories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling:   0%|          | 0/5000 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bszh/MILE/train.py\", line 133, in <module>\n",
      "    train_bde(cfg, n_devices)\n",
      "  File \"/home/bszh/MILE/train.py\", line 19, in train_bde\n",
      "    trainer.train_bde()\n",
      "  File \"/home/bszh/MILE/src/training/trainer.py\", line 177, in train_bde\n",
      "    self.start_sampling()\n",
      "  File \"/usr/lib/python3.12/contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bszh/MILE/src/training/trainer.py\", line 599, in start_sampling\n",
      "    inference_loop_batch(\n",
      "  File \"/home/bszh/MILE/src/training/sampling_batch.py\", line 266, in inference_loop_batch\n",
      "    _log_metrics(step_count, state, batch)\n",
      "  File \"/home/bszh/MILE/src/training/sampling_batch.py\", line 217, in _log_metrics\n",
      "    curr_logpost = logpost(state.params, X, y)\n",
      "                   ^^^^^^^\n",
      "NameError: cannot access free variable 'logpost' where it is not associated with a value in enclosing scope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Running training for config: experiments/csgld/bike/sequential_constant_2_seed2.yaml\n",
      "2025-06-17 21:58:20,163 - __main__ - INFO - Loaded 1 Experiment(s)\n",
      "2025-06-17 21:58:20,695 - datasets - INFO - PyTorch version 2.2.2+cpu available.\n",
      "2025-06-17 21:58:20,696 - datasets - INFO - JAX version 0.4.28 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/bszh/MILE/train.py\", line 133, in <module>\n",
      "    train_bde(cfg, n_devices)\n",
      "  File \"/home/bszh/MILE/train.py\", line 13, in train_bde\n",
      "    from src.training.trainer import BDETrainer  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bszh/MILE/src/training/trainer.py\", line 29, in <module>\n",
      "    from src.inference.reporting import generate_html_report\n",
      "  File \"/home/bszh/MILE/src/inference/reporting.py\", line 7, in <module>\n",
      "    from nbconvert import HTMLExporter\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbconvert/__init__.py\", line 6, in <module>\n",
      "    from . import filters, postprocessors, preprocessors, writers\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbconvert/filters/__init__.py\", line 8, in <module>\n",
      "    from .markdown import (\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbconvert/filters/markdown.py\", line 12, in <module>\n",
      "    from .markdown_mistune import markdown2html_mistune\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbconvert/filters/markdown_mistune.py\", line 21, in <module>\n",
      "    from nbconvert.filters.strings import add_anchor\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbconvert/filters/strings.py\", line 23, in <module>\n",
      "    from nbconvert.preprocessors.sanitize import _get_default_css_sanitizer\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbconvert/preprocessors/__init__.py\", line 3, in <module>\n",
      "    from nbclient.exceptions import CellExecutionError\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbclient/__init__.py\", line 2, in <module>\n",
      "    from .client import NotebookClient, execute\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/nbclient/client.py\", line 17, in <module>\n",
      "    from jupyter_client.client import KernelClient\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/jupyter_client/__init__.py\", line 3, in <module>\n",
      "    from .asynchronous import AsyncKernelClient\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/jupyter_client/asynchronous/__init__.py\", line 1, in <module>\n",
      "    from .client import AsyncKernelClient  # noqa\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/jupyter_client/asynchronous/client.py\", line 11, in <module>\n",
      "    from ..channels import AsyncZMQSocketChannel, HBChannel\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/jupyter_client/channels.py\", line 16, in <module>\n",
      "    from .session import Session\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/jupyter_client/session.py\", line 31, in <module>\n",
      "    from tornado.ioloop import IOLoop\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/tornado/ioloop.py\", line 39, in <module>\n",
      "    from tornado.concurrent import (\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/tornado/concurrent.py\", line 34, in <module>\n",
      "    from tornado.log import app_log\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/tornado/log.py\", line 34, in <module>\n",
      "    from tornado.escape import _unicode\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/tornado/escape.py\", line 33, in <module>\n",
      "    from tornado.util import unicode_type\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1322, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1262, in _find_spec\n",
      "  File \"/home/bszh/MILE/venv/lib/python3.12/site-packages/_distutils_hack/__init__.py\", line 105, in find_spec\n",
      "    if path is not None and not fullname.startswith('test.'):\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning training for config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m12\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/lib/python3.12/subprocess.py:1201\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.12/subprocess.py:2053\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/lib/python3.12/subprocess.py:2011\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sequential\n",
    "chains_cycles = [2, 4, 8, 12]\n",
    "config_paths = []\n",
    "for i, n_cycles in enumerate(chains_cycles):\n",
    "    for seed in range(3):\n",
    "        exp_name = f'bike/sequential_constant_{n_cycles}_seed{seed}'\n",
    "        config_path = get_config(\n",
    "            exp_name=exp_name,\n",
    "            n_chains=1,\n",
    "            n_cycles=n_cycles,\n",
    "            n_steps_per_cycle=2500,\n",
    "            n_samples_per_cycle=500,\n",
    "            n_thinning=10,\n",
    "            optimizer_name='adam',\n",
    "            scheduler_name='Constant',\n",
    "            step_size_init=0.01,\n",
    "            step_size_sampling=1.0e-8,\n",
    "            seed=seed\n",
    "        )\n",
    "        # print(f'Config saved to {config_path}')\n",
    "        config_paths.append(config_path)\n",
    "\n",
    "for config_path in config_paths:\n",
    "    print(\"=\" * 50)\n",
    "    print(f'Running training for config: {config_path}')\n",
    "    subprocess.run(['python', 'train.py', '-c', str(config_path), '-d', '12'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
